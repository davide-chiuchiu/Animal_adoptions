{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pet adoptions with deep networks\n",
    "This simple project aims to develop the basic understanding how to build a deep neural network with tensorflow by predicting adoptions of animals. It is also a way for me to play around with tensorflow functionalities, and to have a nice fallback example when I have problems with larger projects.\n",
    "\n",
    "Let's start by importing relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import complete libraries\n",
    "import numpy\n",
    "import pandas\n",
    "import tensorflow\n",
    "\n",
    "# import sub-libraries and specific functions\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset with the keras get_file utility, and import it as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = 'http://storage.googleapis.com/download.tensorflow.org/data/petfinder-mini.zip'\n",
    "csv_file = 'datasets/petfinder-mini/petfinder-mini.csv'\n",
    "\n",
    "tensorflow.keras.utils.get_file('petfinder_mini.zip', dataset_url,\n",
    "                        extract=True, cache_dir='.')\n",
    "dataframe = pandas.read_csv(csv_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct labels upon using the information that AdoptionSpeed = 4 labels animals that were not adopted, and drop columns of no interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode data labels\n",
    "dataframe['target'] = numpy.where(dataframe['AdoptionSpeed']==4, 0, 1)\n",
    "\n",
    "# Drop un-used columns.\n",
    "dataframe = dataframe.drop(columns=['AdoptionSpeed', 'Description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset into train, validation and test datasets. I am "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7383 train examples\n",
      "1846 validation examples\n",
      "2308 test examples\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(dataframe, test_size=0.2, random_state = 0)\n",
    "train, val = train_test_split(train, test_size=0.2, random_state = 0)\n",
    "print(len(train), 'train examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create datasets from dataframe using utilities from the GCP platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop('target')\n",
    "  ds = tensorflow.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  return ds\n",
    "\n",
    "batch_size = 32\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And below, just a few extra utilities that helps with the job of inspecting stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type                : [b'Dog' b'Cat' b'Cat' b'Dog' b'Dog' b'Dog' b'Cat' b'Cat' b'Cat' b'Cat'\n",
      " b'Dog' b'Cat' b'Dog' b'Cat' b'Cat' b'Dog' b'Dog' b'Dog' b'Cat' b'Dog'\n",
      " b'Dog' b'Dog' b'Cat' b'Dog' b'Cat' b'Dog' b'Cat' b'Cat' b'Dog' b'Cat'\n",
      " b'Dog' b'Cat']\n",
      "Age                 : [ 2  1  3 12 16 28  3  3 36  2  1  2 12  9  3  7 77 12  3  3 12  1  2 12\n",
      "  2  7  1  2  2  3  2  1]\n",
      "Breed1              : [b'Mixed Breed' b'Domestic Medium Hair' b'Domestic Medium Hair' b'Poodle'\n",
      " b'Pit Bull Terrier' b'Mixed Breed' b'Domestic Medium Hair'\n",
      " b'Domestic Medium Hair' b'Domestic Short Hair' b'Domestic Short Hair'\n",
      " b'Mixed Breed' b'Domestic Medium Hair' b'Mixed Breed' b'Turkish Van'\n",
      " b'Domestic Long Hair' b'Mixed Breed' b'Shih Tzu' b'Mixed Breed'\n",
      " b'Domestic Short Hair' b'Mixed Breed' b'Mixed Breed'\n",
      " b'Labrador Retriever' b'Domestic Short Hair' b'Mixed Breed'\n",
      " b'Domestic Short Hair' b'Mixed Breed' b'Persian' b'Domestic Short Hair'\n",
      " b'Mixed Breed' b'Domestic Medium Hair' b'Mixed Breed'\n",
      " b'Domestic Short Hair']\n",
      "Gender              : [b'Female' b'Female' b'Female' b'Male' b'Female' b'Male' b'Male' b'Male'\n",
      " b'Female' b'Female' b'Female' b'Female' b'Female' b'Male' b'Male' b'Male'\n",
      " b'Female' b'Male' b'Male' b'Female' b'Male' b'Female' b'Female' b'Male'\n",
      " b'Male' b'Female' b'Male' b'Male' b'Female' b'Female' b'Female' b'Female']\n",
      "Color1              : [b'Black' b'Brown' b'Cream' b'Cream' b'Brown' b'Black' b'Yellow' b'Black'\n",
      " b'Black' b'Brown' b'Brown' b'Black' b'Brown' b'Yellow' b'Brown' b'Golden'\n",
      " b'Brown' b'Brown' b'Black' b'Black' b'Brown' b'Black' b'Gray' b'Brown'\n",
      " b'Golden' b'Brown' b'Yellow' b'Black' b'Cream' b'Black' b'Black' b'Black']\n",
      "Color2              : [b'Brown' b'Gray' b'Gray' b'No Color' b'White' b'Brown' b'No Color'\n",
      " b'White' b'Yellow' b'Cream' b'No Color' b'Yellow' b'No Color' b'White'\n",
      " b'White' b'No Color' b'White' b'No Color' b'Gray' b'Brown' b'Golden'\n",
      " b'Brown' b'White' b'Golden' b'Gray' b'No Color' b'No Color' b'Gray'\n",
      " b'No Color' b'Brown' b'Brown' b'Brown']\n",
      "MaturitySize        : [b'Medium' b'Medium' b'Medium' b'Small' b'Large' b'Large' b'Small'\n",
      " b'Medium' b'Medium' b'Medium' b'Medium' b'Medium' b'Small' b'Large'\n",
      " b'Medium' b'Medium' b'Medium' b'Medium' b'Medium' b'Medium' b'Medium'\n",
      " b'Medium' b'Medium' b'Large' b'Medium' b'Medium' b'Small' b'Small'\n",
      " b'Medium' b'Medium' b'Medium' b'Medium']\n",
      "FurLength           : [b'Short' b'Medium' b'Medium' b'Medium' b'Short' b'Medium' b'Medium'\n",
      " b'Medium' b'Short' b'Short' b'Short' b'Medium' b'Short' b'Medium' b'Long'\n",
      " b'Short' b'Short' b'Medium' b'Short' b'Medium' b'Medium' b'Short'\n",
      " b'Short' b'Short' b'Short' b'Medium' b'Medium' b'Short' b'Medium'\n",
      " b'Medium' b'Medium' b'Short']\n",
      "Vaccinated          : [b'Not Sure' b'No' b'Yes' b'No' b'Not Sure' b'Not Sure' b'Not Sure' b'Yes'\n",
      " b'No' b'No' b'Yes' b'No' b'Yes' b'Yes' b'No' b'Yes' b'Yes' b'Yes' b'No'\n",
      " b'Yes' b'Yes' b'No' b'No' b'Yes' b'No' b'Yes' b'No' b'Not Sure' b'No'\n",
      " b'No' b'Yes' b'No']\n",
      "Sterilized          : [b'No' b'No' b'No' b'No' b'Yes' b'Not Sure' b'Not Sure' b'No' b'No' b'No'\n",
      " b'No' b'No' b'Yes' b'Not Sure' b'No' b'Yes' b'Yes' b'Yes' b'No' b'No'\n",
      " b'Yes' b'No' b'No' b'No' b'No' b'Yes' b'No' b'Not Sure' b'No' b'No' b'No'\n",
      " b'No']\n",
      "Health              : [b'Healthy' b'Healthy' b'Healthy' b'Healthy' b'Healthy' b'Healthy'\n",
      " b'Healthy' b'Healthy' b'Healthy' b'Healthy' b'Healthy' b'Healthy'\n",
      " b'Healthy' b'Healthy' b'Healthy' b'Healthy' b'Healthy' b'Healthy'\n",
      " b'Healthy' b'Healthy' b'Healthy' b'Healthy' b'Healthy' b'Healthy'\n",
      " b'Healthy' b'Healthy' b'Healthy' b'Healthy' b'Healthy' b'Healthy'\n",
      " b'Healthy' b'Healthy']\n",
      "Fee                 : [  0   0   0   0 200   0   0   0   0   0   0   0   0 100  38   0   0   0\n",
      "   0   0   0   0  50   0   0   0   0   0   0   0   0   0]\n",
      "PhotoAmt            : [ 1  5  3  3  2  2  4 14  5  4 24  3  2  5  4  2  1  1 10  5  0  1  2  2\n",
      "  1  3  6  2  5  1  2  7]\n",
      "label               : [1 1 1 1 1 0 1 0 0 1 1 1 1 0 1 1 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# extract one batch to play around\n",
    "batch, label = iter(train_ds).next()\n",
    "\n",
    "# Utility to visualize the dataset structure\n",
    "for key, value in batch.items():\n",
    "    print(f\"{key:20s}: {value}\")\n",
    "print(f\"{'label':20s}: {label}\")\n",
    "\n",
    "# utility to inspect the dataset composition\n",
    "def demo(feature_column):\n",
    "  feature_layer = layers.DenseFeatures(feature_column)\n",
    "  print(feature_layer(batch).numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, now I can start to play around by building the feature column. This means that I will combine diffeten features together. First, let's create the groups of basic features that I want to include."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# purely numeric features\n",
    "numeric_features = ['PhotoAmt', \n",
    "                    'Fee']\n",
    "\n",
    "# bucketized features, with buckets to use in a feature:bucket dictionary form\n",
    "bucketized_features = {'Age': [1, 2, 3, 4, 5]}\n",
    "\n",
    "# indicator features\n",
    "indicator_features = ['Type', \n",
    "                      'Color1', \n",
    "                      'Color2', \n",
    "                      'Gender', \n",
    "                      'MaturitySize',\n",
    "                      'FurLength', \n",
    "                      'Vaccinated', \n",
    "                      'Sterilized', \n",
    "                      'Health']\n",
    "\n",
    "# embedded features\n",
    "embedded_features = ['Breed1']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's define the feature columns. Note that you can apply the demo utility on each new_feature separately, or on the overall feature_columns array as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "inspect everything\n",
      "WARNING:tensorflow:From /Users/dabol99/opt/anaconda3/envs/py_37/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4276: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /Users/dabol99/opt/anaconda3/envs/py_37/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4331: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "[[0. 0. 1. ... 0. 1. 0.]\n",
      " [0. 1. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 1.]\n",
      " [0. 1. 0. ... 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "feature_columns = []\n",
    "\n",
    "# add numeric features\n",
    "for feature_name in numeric_features:\n",
    "    new_feature = feature_column.numeric_column(feature_name)\n",
    "    feature_columns.append(new_feature)\n",
    "    \n",
    "# add bucketized features from numeric    \n",
    "for feature_name in bucketized_features:\n",
    "    new_feature = feature_column.bucketized_column(feature_column.numeric_column(feature_name),\n",
    "                                                   bucketized_features[feature_name])\n",
    "    feature_columns.append(new_feature)\n",
    "       \n",
    "# add indicator feature\n",
    "for feature_name in indicator_features:\n",
    "    new_feature_as_categorical = feature_column.categorical_column_with_vocabulary_list(feature_name, dataframe[feature_name].unique())\n",
    "    new_feature_as_indicator   = feature_column.indicator_column(new_feature_as_categorical) \n",
    "    feature_columns.append(new_feature_as_indicator)\n",
    "\n",
    "# add embedded features\n",
    "for feature_name in embedded_features:\n",
    "    print(1)\n",
    "    naive_embedding_size = int(numpy.round(len(dataframe[feature_name].unique())**(0.25)))\n",
    "    new_feature_as_categorical = feature_column.categorical_column_with_vocabulary_list(feature_name, dataframe[feature_name].unique())\n",
    "    new_feature_as_embedding   = feature_column.embedding_column(new_feature_as_categorical, naive_embedding_size)\n",
    "    feature_columns.append(new_feature_as_embedding)\n",
    "    \n",
    "print('inspect everything')\n",
    "demo(feature_columns)\n",
    "# Warnings comes out because conda on macOS can have only tensorflow 2.0.0 an not 2.3.14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that feature columns are ready, I can now finally train a deep neural network model, albeit a very simple one. Let's also play with some hyperparameter tuning, so that I can chose a best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of nodes to try: integer values from 16 to 32. \n",
    "HP_NUM_NODES = hp.HParam('num_units', hp.Discrete([16, 32]))\n",
    "\n",
    "hyperparameters = {'num_nodes': 128,\n",
    "                   'dropout': 0.1,\n",
    "                   'epochs': 5,\n",
    "                   'optimizer' : 'ftrl',\n",
    "                   'metrics' : ['accuracy', \n",
    "                                tensorflow.keras.metrics.Recall(),\n",
    "                                tensorflow.keras.metrics.Precision()]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_nodes': 128, 'dropout': 0.1, 'epochs': 5, 'optimizer': 'ftrl', 'metrics': ['accuracy', <tensorflow.python.keras.metrics.Recall object at 0x7f99034f4650>, <tensorflow.python.keras.metrics.Precision object at 0x7f99034e1d10>]}\n"
     ]
    }
   ],
   "source": [
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "231/231 [==============================] - 10s 43ms/step - loss: 0.6064 - accuracy: 0.7334 - recall: 0.9959 - precision: 0.7353 - val_loss: 0.0000e+00 - val_accuracy: 0.0000e+00 - val_recall: 0.0000e+00 - val_precision: 0.0000e+00\n",
      "Epoch 2/5\n",
      "231/231 [==============================] - 4s 15ms/step - loss: 0.6060 - accuracy: 0.7351 - recall: 1.0000 - precision: 0.7351 - val_loss: 0.6038 - val_accuracy: 0.7389 - val_recall: 1.0000 - val_precision: 0.7389\n",
      "Epoch 3/5\n",
      "231/231 [==============================] - 3s 14ms/step - loss: 0.6058 - accuracy: 0.7351 - recall: 1.0000 - precision: 0.7351 - val_loss: 0.6035 - val_accuracy: 0.7389 - val_recall: 1.0000 - val_precision: 0.7389\n",
      "Epoch 4/5\n",
      "231/231 [==============================] - 4s 16ms/step - loss: 0.6058 - accuracy: 0.7351 - recall: 1.0000 - precision: 0.7351 - val_loss: 0.6034 - val_accuracy: 0.7389 - val_recall: 1.0000 - val_precision: 0.7389\n",
      "Epoch 5/5\n",
      "231/231 [==============================] - 4s 16ms/step - loss: 0.6055 - accuracy: 0.7351 - recall: 1.0000 - precision: 0.7351 - val_loss: 0.6032 - val_accuracy: 0.7389 - val_recall: 1.0000 - val_precision: 0.7389\n",
      "58/58 [==============================] - 1s 10ms/step - loss: 0.6032 - accuracy: 0.7389 - recall: 1.0000 - precision: 0.7389\n",
      "0.6032058843250933\n",
      "0.7388949\n",
      "1.0\n",
      "0.7388949\n"
     ]
    }
   ],
   "source": [
    "def train_and_test_model(feature_columns, train_ds, val_ds, hyperparameters):    \n",
    "    # biuild input layer from feature columns\n",
    "    input_layer = tensorflow.keras.layers.DenseFeatures(feature_columns)\n",
    "\n",
    "    # build sequential model\n",
    "    model = tensorflow.keras.Sequential([\n",
    "      input_layer,\n",
    "      layers.Dense(hyperparameters['num_nodes'], activation='relu'),\n",
    "      layers.Dense(hyperparameters['num_nodes'], activation='relu'),\n",
    "      layers.Dropout(hyperparameters['dropout']),\n",
    "      # sigmoid layer to perfomr classification tasks\n",
    "      layers.Dense(1,  activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # compile model\n",
    "    model.compile(optimizer = hyperparameters['optimizer'],\n",
    "                  loss = tensorflow.keras.losses.BinaryCrossentropy(from_logits = True),\n",
    "                  metrics = hyperparameters['metrics'])\n",
    "\n",
    "    # train model\n",
    "    model.fit(train_ds,\n",
    "              validation_data = val_ds,\n",
    "              epochs = hyperparameters['epochs'])\n",
    "    \n",
    "    # evaluate model\n",
    "    loss, accuracy, recall, precision = model.evaluate(val_ds)\n",
    "    \n",
    "    return model, loss, accuracy, recall, precision\n",
    "\n",
    "model, loss, accuracy, recall, precision = train_and_test_model(feature_columns, train_ds, val_ds, hyperparameters)\n",
    "print(loss)\n",
    "print(accuracy)\n",
    "print(recall)\n",
    "print(precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
