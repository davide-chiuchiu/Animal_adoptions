{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pet adoptions with deep networks\n",
    "This simple project aims to build a deep neural network with tensorflow by predicting adoptions of animals. It is also a way for me to play around with tensorflow functionalities, and to have a nice fallback example when I have problems with larger projects. The project follows https://www.tensorflow.org/tutorials/structured_data/feature_columns, with hyperparameter tuning inspired by https://www.tensorflow.org/tutorials/keras/keras_tuner\n",
    "### Library import\n",
    "Let's start by importing relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import complete libraries\n",
    "import numpy\n",
    "import pandas\n",
    "import tensorflow\n",
    "import kerastuner\n",
    "import IPython\n",
    "\n",
    "# import sub-libraries and specific functions\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import and create train, validate, test dataset\n",
    "Download the dataset with the keras get_file utility, and import it as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = 'http://storage.googleapis.com/download.tensorflow.org/data/petfinder-mini.zip'\n",
    "csv_file = 'datasets/petfinder-mini/petfinder-mini.csv'\n",
    "\n",
    "tensorflow.keras.utils.get_file('petfinder_mini.zip', dataset_url,\n",
    "                        extract=True, cache_dir='.')\n",
    "dataframe = pandas.read_csv(csv_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct labels upon using the information that AdoptionSpeed = 4 labels animals that were not adopted, and drop columns of no interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode data labels\n",
    "dataframe['target'] = numpy.where(dataframe['AdoptionSpeed']==4, 0, 1)\n",
    "\n",
    "# Drop un-used columns.\n",
    "dataframe = dataframe.drop(columns=['AdoptionSpeed', 'Description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset into train, validation and test datasets. I am "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7383 train examples\n",
      "1846 validation examples\n",
      "2308 test examples\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(dataframe, test_size=0.2, random_state = 0)\n",
    "train, val = train_test_split(train, test_size=0.2, random_state = 0)\n",
    "print(len(train), 'train examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create datasets from dataframe using utilities from the GCP platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop('target')\n",
    "  ds = tensorflow.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  return ds\n",
    "\n",
    "batch_size = 32\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And below, just a few extra utilities that helps with the job of inspecting stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type                : [b'Cat' b'Dog' b'Cat' b'Cat' b'Dog' b'Cat' b'Cat' b'Dog' b'Cat' b'Dog'\n",
      " b'Cat' b'Dog' b'Cat' b'Cat' b'Cat' b'Dog' b'Dog' b'Cat' b'Cat' b'Dog'\n",
      " b'Dog' b'Cat' b'Dog' b'Dog' b'Dog' b'Dog' b'Dog' b'Dog' b'Dog' b'Dog'\n",
      " b'Cat' b'Dog']\n",
      "Age                 : [ 4  3 12 12  8  3 60 12  3  6  2  2  1  2  6  3 12  3  2 53  4 12  2  2\n",
      "  5 18 24  3  2  1  5  2]\n",
      "Breed1              : [b'Domestic Short Hair' b'Mixed Breed' b'Domestic Short Hair'\n",
      " b'Domestic Long Hair' b'Corgi' b'Domestic Short Hair' b'Persian' b'Spitz'\n",
      " b'Domestic Short Hair' b'Dachshund' b'Domestic Short Hair' b'Mixed Breed'\n",
      " b'Domestic Medium Hair' b'Calico' b'Domestic Short Hair' b'Mixed Breed'\n",
      " b'Miniature Pinscher' b'Domestic Short Hair' b'Domestic Short Hair'\n",
      " b'Maltese' b'Mixed Breed' b'Tabby' b'Mixed Breed' b'Mixed Breed' b'Spitz'\n",
      " b'Shih Tzu' b'Mixed Breed' b'Miniature Pinscher' b'Jack Russell Terrier'\n",
      " b'Mixed Breed' b'Domestic Medium Hair' b'Mixed Breed']\n",
      "Gender              : [b'Male' b'Female' b'Female' b'Female' b'Female' b'Male' b'Male' b'Female'\n",
      " b'Male' b'Female' b'Male' b'Female' b'Male' b'Female' b'Female' b'Female'\n",
      " b'Female' b'Female' b'Female' b'Male' b'Male' b'Male' b'Male' b'Male'\n",
      " b'Female' b'Male' b'Male' b'Male' b'Female' b'Male' b'Male' b'Male']\n",
      "Color1              : [b'Golden' b'Black' b'Golden' b'Gray' b'Black' b'Brown' b'Golden' b'Brown'\n",
      " b'Yellow' b'Brown' b'Black' b'Cream' b'Black' b'Black' b'Black' b'Brown'\n",
      " b'Black' b'Yellow' b'Brown' b'White' b'Cream' b'Yellow' b'Cream' b'Black'\n",
      " b'Brown' b'Brown' b'Brown' b'Brown' b'Brown' b'Black' b'Golden' b'Cream']\n",
      "Color2              : [b'No Color' b'Brown' b'No Color' b'White' b'No Color' b'Gray' b'No Color'\n",
      " b'Golden' b'White' b'No Color' b'White' b'White' b'White' b'Brown'\n",
      " b'Brown' b'No Color' b'Yellow' b'Gray' b'White' b'No Color' b'No Color'\n",
      " b'Cream' b'No Color' b'Brown' b'White' b'Cream' b'White' b'No Color'\n",
      " b'White' b'Brown' b'White' b'No Color']\n",
      "MaturitySize        : [b'Medium' b'Medium' b'Medium' b'Medium' b'Small' b'Small' b'Medium'\n",
      " b'Small' b'Small' b'Medium' b'Small' b'Small' b'Small' b'Medium'\n",
      " b'Medium' b'Large' b'Medium' b'Large' b'Small' b'Small' b'Medium'\n",
      " b'Medium' b'Medium' b'Medium' b'Medium' b'Small' b'Medium' b'Small'\n",
      " b'Medium' b'Medium' b'Large' b'Medium']\n",
      "FurLength           : [b'Medium' b'Short' b'Short' b'Medium' b'Short' b'Short' b'Medium'\n",
      " b'Medium' b'Short' b'Short' b'Medium' b'Short' b'Medium' b'Medium'\n",
      " b'Short' b'Short' b'Short' b'Short' b'Short' b'Long' b'Short' b'Short'\n",
      " b'Medium' b'Medium' b'Medium' b'Medium' b'Short' b'Short' b'Short'\n",
      " b'Short' b'Medium' b'Short']\n",
      "Vaccinated          : [b'No' b'No' b'Yes' b'Yes' b'Yes' b'No' b'Yes' b'Not Sure' b'No' b'Yes'\n",
      " b'No' b'Yes' b'No' b'No' b'No' b'No' b'Yes' b'Yes' b'No' b'Yes' b'Yes'\n",
      " b'No' b'No' b'Yes' b'Not Sure' b'Yes' b'Not Sure' b'Yes' b'Yes' b'No'\n",
      " b'Not Sure' b'No']\n",
      "Sterilized          : [b'No' b'No' b'Yes' b'No' b'No' b'No' b'No' b'Not Sure' b'No' b'Yes' b'No'\n",
      " b'No' b'No' b'No' b'Yes' b'No' b'No' b'Yes' b'No' b'Yes' b'Yes' b'Yes'\n",
      " b'No' b'No' b'Yes' b'Yes' b'Not Sure' b'Not Sure' b'No' b'No' b'No' b'No']\n",
      "Health              : [b'Healthy' b'Healthy' b'Healthy' b'Healthy' b'Healthy' b'Healthy'\n",
      " b'Healthy' b'Healthy' b'Healthy' b'Healthy' b'Healthy' b'Healthy'\n",
      " b'Healthy' b'Healthy' b'Healthy' b'Healthy' b'Healthy' b'Healthy'\n",
      " b'Healthy' b'Healthy' b'Healthy' b'Healthy' b'Healthy' b'Healthy'\n",
      " b'Healthy' b'Minor Injury' b'Healthy' b'Healthy' b'Healthy' b'Healthy'\n",
      " b'Healthy' b'Healthy']\n",
      "Fee                 : [  0   0  50   0   0   0   0   0   0   0   0  50   0   0   0   0 250 100\n",
      "  50 100   0 200   0   0   0   0   0 350   0   0   0 150]\n",
      "PhotoAmt            : [ 5  4  1  3  4  6  0  5  1  3  3  3  1  4  7  6  3  2  1  4  3  1  1  6\n",
      "  1  1  2  5  9 11  1 16]\n",
      "label               : [1 0 1 1 0 1 0 1 1 0 1 1 1 1 1 0 1 0 1 1 0 0 1 1 1 1 0 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# extract one batch to play around\n",
    "batch, label = iter(train_ds).next()\n",
    "\n",
    "# Utility to visualize the dataset structure\n",
    "for key, value in batch.items():\n",
    "    print(f\"{key:20s}: {value}\")\n",
    "print(f\"{'label':20s}: {label}\")\n",
    "\n",
    "# utility to inspect the dataset composition\n",
    "def demo(feature_column):\n",
    "  feature_layer = layers.DenseFeatures(feature_column)\n",
    "  print(feature_layer(batch).numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build feature columns\n",
    "Okay, now I can start to play around by building the feature column. This means that I will combine diffeten features together. First, let's create the groups of basic features that I want to include."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# purely numeric features\n",
    "numeric_features = ['PhotoAmt', \n",
    "                    'Fee']\n",
    "\n",
    "# bucketized features, with buckets to use in a feature:bucket dictionary form\n",
    "bucketized_features = {'Age': [1, 2, 3, 4, 5]}\n",
    "\n",
    "# indicator features\n",
    "indicator_features = ['Type', \n",
    "                      'Color1', \n",
    "                      'Color2', \n",
    "                      'Gender', \n",
    "                      'MaturitySize',\n",
    "                      'FurLength', \n",
    "                      'Vaccinated', \n",
    "                      'Sterilized', \n",
    "                      'Health']\n",
    "\n",
    "# embedded features\n",
    "embedded_features = ['Breed1']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's define the feature columns. Note that you can apply the demo utility on each new_feature separately, or on the overall feature_columns array as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inspect everything\n",
      "WARNING:tensorflow:From /Users/dabol99/opt/anaconda3/envs/py_37/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4276: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /Users/dabol99/opt/anaconda3/envs/py_37/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4331: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [0. 1. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 1. 0.]\n",
      " [0. 0. 1. ... 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "feature_columns = []\n",
    "\n",
    "# add numeric features\n",
    "for feature_name in numeric_features:\n",
    "    new_feature = feature_column.numeric_column(feature_name)\n",
    "    feature_columns.append(new_feature)\n",
    "    \n",
    "# add bucketized features from numeric    \n",
    "for feature_name in bucketized_features:\n",
    "    new_feature = feature_column.bucketized_column(feature_column.numeric_column(feature_name),\n",
    "                                                   bucketized_features[feature_name])\n",
    "    feature_columns.append(new_feature)\n",
    "       \n",
    "# add indicator feature\n",
    "for feature_name in indicator_features:\n",
    "    new_feature_as_categorical = feature_column.categorical_column_with_vocabulary_list(feature_name, dataframe[feature_name].unique())\n",
    "    new_feature_as_indicator   = feature_column.indicator_column(new_feature_as_categorical) \n",
    "    feature_columns.append(new_feature_as_indicator)\n",
    "\n",
    "# add embedded features\n",
    "for feature_name in embedded_features:\n",
    "    naive_embedding_size = int(numpy.round(len(dataframe[feature_name].unique())**(0.25)))\n",
    "    new_feature_as_categorical = feature_column.categorical_column_with_vocabulary_list(feature_name, dataframe[feature_name].unique())\n",
    "    new_feature_as_embedding   = feature_column.embedding_column(new_feature_as_categorical, naive_embedding_size)\n",
    "    feature_columns.append(new_feature_as_embedding)\n",
    "    \n",
    "print('inspect everything')\n",
    "demo(feature_columns)\n",
    "# Warnings comes out because conda on macOS can have only tensorflow 2.0.0 an not 2.3.14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural model\n",
    "Now that feature columns are ready, I can now finally train a deep neural network model, albeit a very simple one. Let's start by writing down a funcitons that initialized a model object that I can feed to the hyperparameter tuner. Note that in this function the hyperparameters object does not have specific details on the hyperparameter space. Those are defined within the function initialize_model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(hyperparameters):    \n",
    "    # build input layer from feature columns\n",
    "    input_layer = tensorflow.keras.layers.DenseFeatures(feature_columns)\n",
    "    \n",
    "    # specify hyperparameter ranges hyperparameter_object \n",
    "    node_units  = hyperparameters.Int('units', min_value = 10, max_value = 50, step = 10)\n",
    "    dropout_val = hyperparameters.Float('dropout', min_value = 0.1, max_value = 0.25, step = 0.05)\n",
    "    optimizers  = hyperparameters.Choice('optimizer', ['adam', 'ftrl'])\n",
    "    \n",
    "    # build sequential model\n",
    "    model = tensorflow.keras.Sequential([\n",
    "      input_layer,\n",
    "      layers.Dense(units = node_units, activation='relu'),\n",
    "      layers.Dropout(rate = dropout_val),\n",
    "      layers.Dense(units = node_units, activation='relu'),\n",
    "      layers.Dropout(rate = dropout_val),\n",
    "      # sigmoid layer to perfomr classification tasks\n",
    "      layers.Dense(1,  activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # compile model\n",
    "    model.compile(optimizer = 'adam',\n",
    "                  loss = tensorflow.keras.losses.BinaryCrossentropy(from_logits = True),\n",
    "                  metrics = ['accuracy',\n",
    "                            tensorflow.keras.metrics.Precision(name='precision'),\n",
    "                            tensorflow.keras.metrics.Recall(name='recall')])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use this initializer function to set up an hyperparameter tuner together with the callback class that clears training outputs after every run of the model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kerastuner.Hyperband(initialize_model,\n",
    "                             objective = 'val_accuracy', \n",
    "                             max_epochs = 10,\n",
    "                             factor = 3,\n",
    "                             directory = 'logs',\n",
    "                             project_name = 'hyperparameter_tuning')\n",
    "\n",
    "class ClearTrainingOutput(tensorflow.keras.callbacks.Callback):\n",
    "  def on_train_end(*args, **kwargs):\n",
    "    IPython.display.clear_output(wait = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: f6f41d8278f2b946aef0fce26ee9a5ef</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 0.7367280721664429</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-dropout: 0.15000000000000002</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-optimizer: adam</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-tuner/bracket: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-tuner/epochs: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-tuner/initial_epoch: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-tuner/round: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-units: 30</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "     74/Unknown - 4s 4s/step - loss: 0.6535 - accuracy: 0.4375 - precision: 0.6316 - recall: 0.521 - 4s 2s/step - loss: 0.6339 - accuracy: 0.4844 - precision: 0.6750 - recall: 0.574 - 4s 1s/step - loss: 0.5795 - accuracy: 0.6042 - precision: 0.7826 - recall: 0.701 - 4s 892ms/step - loss: 0.5982 - accuracy: 0.6016 - precision: 0.7423 - recall: 0.73 - 4s 720ms/step - loss: 0.6026 - accuracy: 0.6187 - precision: 0.7323 - recall: 0.77 - 4s 604ms/step - loss: 0.5930 - accuracy: 0.6562 - precision: 0.7532 - recall: 0.81 - 4s 522ms/step - loss: 0.5918 - accuracy: 0.6652 - precision: 0.7540 - recall: 0.82 - 4s 460ms/step - loss: 0.5953 - accuracy: 0.6719 - precision: 0.7489 - recall: 0.84 - 4s 413ms/step - loss: 0.5884 - accuracy: 0.6840 - precision: 0.7560 - recall: 0.86 - 4s 374ms/step - loss: 0.5967 - accuracy: 0.6781 - precision: 0.7411 - recall: 0.87 - 4s 342ms/step - loss: 0.6043 - accuracy: 0.6733 - precision: 0.7293 - recall: 0.88 - 4s 315ms/step - loss: 0.6130 - accuracy: 0.6641 - precision: 0.7139 - recall: 0.89 - 4s 293ms/step - loss: 0.6098 - accuracy: 0.6707 - precision: 0.7169 - recall: 0.90 - 4s 274ms/step - loss: 0.5987 - accuracy: 0.6897 - precision: 0.7341 - recall: 0.90 - 4s 257ms/step - loss: 0.6033 - accuracy: 0.6833 - precision: 0.7256 - recall: 0.91 - 4s 243ms/step - loss: 0.6014 - accuracy: 0.6855 - precision: 0.7267 - recall: 0.91 - 4s 229ms/step - loss: 0.6010 - accuracy: 0.6875 - precision: 0.7262 - recall: 0.91 - 4s 218ms/step - loss: 0.6034 - accuracy: 0.6875 - precision: 0.7239 - recall: 0.92 - 4s 208ms/step - loss: 0.6021 - accuracy: 0.6908 - precision: 0.7254 - recall: 0.92 - 4s 200ms/step - loss: 0.5947 - accuracy: 0.7016 - precision: 0.7350 - recall: 0.93 - 4s 192ms/step - loss: 0.5992 - accuracy: 0.6964 - precision: 0.7278 - recall: 0.93 - 4s 185ms/step - loss: 0.5958 - accuracy: 0.7017 - precision: 0.7319 - recall: 0.93 - 4s 178ms/step - loss: 0.5997 - accuracy: 0.6984 - precision: 0.7270 - recall: 0.94 - 4s 172ms/step - loss: 0.5943 - accuracy: 0.7057 - precision: 0.7335 - recall: 0.94 - 4s 166ms/step - loss: 0.5900 - accuracy: 0.7113 - precision: 0.7382 - recall: 0.94 - 4s 160ms/step - loss: 0.5910 - accuracy: 0.7103 - precision: 0.7361 - recall: 0.94 - 4s 155ms/step - loss: 0.5916 - accuracy: 0.7106 - precision: 0.7354 - recall: 0.94 - 4s 150ms/step - loss: 0.5914 - accuracy: 0.7109 - precision: 0.7348 - recall: 0.95 - 4s 146ms/step - loss: 0.5913 - accuracy: 0.7112 - precision: 0.7342 - recall: 0.95 - 4s 142ms/step - loss: 0.5913 - accuracy: 0.7115 - precision: 0.7337 - recall: 0.95 - 4s 138ms/step - loss: 0.5903 - accuracy: 0.7127 - precision: 0.7342 - recall: 0.95 - 4s 134ms/step - loss: 0.5883 - accuracy: 0.7158 - precision: 0.7368 - recall: 0.95 - 4s 131ms/step - loss: 0.5897 - accuracy: 0.7150 - precision: 0.7352 - recall: 0.95 - 4s 128ms/step - loss: 0.5885 - accuracy: 0.7169 - precision: 0.7366 - recall: 0.96 - 4s 125ms/step - loss: 0.5872 - accuracy: 0.7188 - precision: 0.7380 - recall: 0.96 - 4s 123ms/step - loss: 0.5849 - accuracy: 0.7214 - precision: 0.7401 - recall: 0.96 - 4s 120ms/step - loss: 0.5847 - accuracy: 0.7221 - precision: 0.7404 - recall: 0.96 - 4s 118ms/step - loss: 0.5843 - accuracy: 0.7229 - precision: 0.7406 - recall: 0.96 - 4s 115ms/step - loss: 0.5843 - accuracy: 0.7228 - precision: 0.7401 - recall: 0.96 - 5s 113ms/step - loss: 0.5848 - accuracy: 0.7219 - precision: 0.7387 - recall: 0.96 - 5s 111ms/step - loss: 0.5860 - accuracy: 0.7210 - precision: 0.7374 - recall: 0.96 - 5s 109ms/step - loss: 0.5846 - accuracy: 0.7225 - precision: 0.7385 - recall: 0.96 - 5s 107ms/step - loss: 0.5843 - accuracy: 0.7231 - precision: 0.7388 - recall: 0.96 - 5s 105ms/step - loss: 0.5871 - accuracy: 0.7202 - precision: 0.7354 - recall: 0.96 - 5s 103ms/step - loss: 0.5862 - accuracy: 0.7215 - precision: 0.7364 - recall: 0.96 - 5s 101ms/step - loss: 0.5879 - accuracy: 0.7201 - precision: 0.7346 - recall: 0.97 - 5s 100ms/step - loss: 0.5864 - accuracy: 0.7221 - precision: 0.7363 - recall: 0.97 - 5s 98ms/step - loss: 0.5864 - accuracy: 0.7220 - precision: 0.7360 - recall: 0.9718 - 5s 97ms/step - loss: 0.5858 - accuracy: 0.7226 - precision: 0.7363 - recall: 0.972 - 5s 95ms/step - loss: 0.5867 - accuracy: 0.7219 - precision: 0.7353 - recall: 0.972 - 5s 94ms/step - loss: 0.5847 - accuracy: 0.7243 - precision: 0.7374 - recall: 0.973 - 5s 93ms/step - loss: 0.5817 - accuracy: 0.7278 - precision: 0.7408 - recall: 0.974 - 5s 92ms/step - loss: 0.5817 - accuracy: 0.7276 - precision: 0.7403 - recall: 0.974 - 5s 90ms/step - loss: 0.5800 - accuracy: 0.7297 - precision: 0.7423 - recall: 0.975 - 5s 89ms/step - loss: 0.5798 - accuracy: 0.7301 - precision: 0.7424 - recall: 0.975 - 5s 87ms/step - loss: 0.5802 - accuracy: 0.7299 - precision: 0.7420 - recall: 0.976 - 5s 86ms/step - loss: 0.5812 - accuracy: 0.7286 - precision: 0.7405 - recall: 0.976 - 5s 85ms/step - loss: 0.5823 - accuracy: 0.7274 - precision: 0.7390 - recall: 0.976 - 5s 84ms/step - loss: 0.5810 - accuracy: 0.7288 - precision: 0.7403 - recall: 0.977 - 5s 83ms/step - loss: 0.5818 - accuracy: 0.7281 - precision: 0.7394 - recall: 0.977 - 5s 82ms/step - loss: 0.5825 - accuracy: 0.7275 - precision: 0.7385 - recall: 0.977 - 5s 81ms/step - loss: 0.5816 - accuracy: 0.7283 - precision: 0.7392 - recall: 0.978 - 5s 80ms/step - loss: 0.5816 - accuracy: 0.7282 - precision: 0.7389 - recall: 0.978 - 5s 79ms/step - loss: 0.5843 - accuracy: 0.7251 - precision: 0.7356 - recall: 0.978 - 5s 78ms/step - loss: 0.5840 - accuracy: 0.7255 - precision: 0.7358 - recall: 0.979 - 5s 77ms/step - loss: 0.5833 - accuracy: 0.7263 - precision: 0.7365 - recall: 0.979 - 5s 77ms/step - loss: 0.5851 - accuracy: 0.7243 - precision: 0.7343 - recall: 0.979 - 5s 76ms/step - loss: 0.5849 - accuracy: 0.7247 - precision: 0.7346 - recall: 0.980 - 5s 75ms/step - loss: 0.5858 - accuracy: 0.7237 - precision: 0.7334 - recall: 0.980 - 5s 75ms/step - loss: 0.5856 - accuracy: 0.7241 - precision: 0.7336 - recall: 0.980 - 5s 74ms/step - loss: 0.5874 - accuracy: 0.7223 - precision: 0.7316 - recall: 0.980 - 5s 73ms/step - loss: 0.5876 - accuracy: 0.7218 - precision: 0.7310 - recall: 0.981 - 5s 72ms/step - loss: 0.5863 - accuracy: 0.7235 - precision: 0.7326 - recall: 0.981 - 5s 72ms/step - loss: 0.5856 - accuracy: 0.7242 - precision: 0.7332 - recall: 0.9816"
     ]
    }
   ],
   "source": [
    "tuner.search(train_ds, epochs = 10, validation_data = val_ds, callbacks = [ClearTrainingOutput()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "print(f\"\"\"nodes:     {best_hps.get('units')}\"\"\")\n",
    "print(f\"\"\"dropout:   {best_hps.get('dropout')}\"\"\")\n",
    "print(f\"\"\"optimizer: {best_hps.get('optmizer')}\"\"\")\n",
    "\n",
    "# Build the model with the optimal hyperparameters and train it on the data\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "model.fit(train_ds, epochs = 10, validation_data = val_ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
