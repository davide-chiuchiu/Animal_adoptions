{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pet adoptions with deep networks\n",
    "This simple project aims to build a deep neural network with tensorflow by predicting adoptions of animals. It is also a way for me to play around with tensorflow functionalities, and to have a nice fallback example when I have problems with larger projects. The project follows https://www.tensorflow.org/tutorials/structured_data/feature_columns, with hyperparameter tuning inspired by https://www.tensorflow.org/tutorials/keras/keras_tuner\n",
    "### Library import\n",
    "Let's start by importing relevant libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import complete libraries\n",
    "import numpy\n",
    "import pandas\n",
    "import tensorflow\n",
    "import kerastuner\n",
    "import os\n",
    "\n",
    "# import sub-libraries and specific functions\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorboard.plugins.hparams import api as hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import and create train, validate, test dataset\n",
    "Download the dataset with the keras get_file utility, and import it as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = 'http://storage.googleapis.com/download.tensorflow.org/data/petfinder-mini.zip'\n",
    "csv_file = 'datasets/petfinder-mini/petfinder-mini.csv'\n",
    "\n",
    "tensorflow.keras.utils.get_file('petfinder_mini.zip', dataset_url,\n",
    "                        extract=True, cache_dir='.')\n",
    "dataframe = pandas.read_csv(csv_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct labels upon using the information that AdoptionSpeed = 4 labels animals that were not adopted, and drop columns of no interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode data labels\n",
    "dataframe['target'] = numpy.where(dataframe['AdoptionSpeed']==4, 0, 1)\n",
    "\n",
    "# Drop un-used columns.\n",
    "dataframe = dataframe.drop(columns=['AdoptionSpeed', 'Description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset into train, validation and test datasets. I am "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7383 train examples\n",
      "1846 validation examples\n",
      "2308 test examples\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(dataframe, test_size=0.2, random_state = 0)\n",
    "train, val = train_test_split(train, test_size=0.2, random_state = 0)\n",
    "print(len(train), 'train examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create datasets from dataframe using utilities from the GCP platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop('target')\n",
    "  ds = tensorflow.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  return ds\n",
    "\n",
    "batch_size = 32\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And below, just a few extra utilities that helps with the job of inspecting stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type                : [b'Dog' b'Dog' b'Cat' b'Dog' b'Dog' b'Dog' b'Cat' b'Cat' b'Cat' b'Dog'\n",
      " b'Dog' b'Dog' b'Dog' b'Cat' b'Dog' b'Dog' b'Cat' b'Cat' b'Cat' b'Cat'\n",
      " b'Dog' b'Dog' b'Dog' b'Cat' b'Cat' b'Dog' b'Cat' b'Cat' b'Dog' b'Dog'\n",
      " b'Dog' b'Dog']\n",
      "Age                 : [ 5  4  1 60 60  3  6  2  3 24  6  2  2  2  2  2  1  4  6  5  1  2 60  2\n",
      "  4  2  2  1  2  4 12  5]\n",
      "Breed1              : [b'Mixed Breed' b'Silky Terrier' b'Domestic Short Hair'\n",
      " b'Golden Retriever' b'Miniature Pinscher' b'Mixed Breed'\n",
      " b'Domestic Short Hair' b'Domestic Short Hair' b'Domestic Short Hair'\n",
      " b'Mixed Breed' b'Mixed Breed' b'Mixed Breed' b'Mixed Breed'\n",
      " b'Domestic Short Hair' b'Mixed Breed' b'Poodle' b'Domestic Medium Hair'\n",
      " b'Domestic Short Hair' b'Domestic Short Hair' b'Domestic Short Hair'\n",
      " b'Mixed Breed' b'Mixed Breed' b'Poodle' b'Domestic Short Hair'\n",
      " b'Domestic Medium Hair' b'Mixed Breed' b'Domestic Short Hair'\n",
      " b'Oriental Long Hair' b'Mixed Breed' b'Mixed Breed' b'Mixed Breed'\n",
      " b'Mixed Breed']\n",
      "Gender              : [b'Male' b'Female' b'Female' b'Male' b'Male' b'Female' b'Male' b'Male'\n",
      " b'Female' b'Male' b'Female' b'Female' b'Male' b'Male' b'Male' b'Female'\n",
      " b'Male' b'Male' b'Female' b'Male' b'Male' b'Female' b'Female' b'Female'\n",
      " b'Female' b'Male' b'Female' b'Female' b'Female' b'Female' b'Female'\n",
      " b'Female']\n",
      "Color1              : [b'Yellow' b'Cream' b'Black' b'Golden' b'Brown' b'Brown' b'White'\n",
      " b'Golden' b'Brown' b'White' b'Brown' b'Black' b'Black' b'Black' b'Black'\n",
      " b'Brown' b'Gray' b'Golden' b'Brown' b'Black' b'Black' b'Brown' b'Cream'\n",
      " b'Black' b'Black' b'Brown' b'Golden' b'Cream' b'Brown' b'Yellow' b'Cream'\n",
      " b'Brown']\n",
      "Color2              : [b'Cream' b'No Color' b'Gray' b'Cream' b'No Color' b'No Color' b'No Color'\n",
      " b'Cream' b'White' b'No Color' b'No Color' b'No Color' b'Brown' b'White'\n",
      " b'White' b'White' b'No Color' b'White' b'No Color' b'No Color' b'White'\n",
      " b'No Color' b'White' b'Brown' b'Gray' b'White' b'White' b'White'\n",
      " b'No Color' b'No Color' b'No Color' b'Golden']\n",
      "MaturitySize        : [b'Medium' b'Medium' b'Medium' b'Large' b'Small' b'Medium' b'Medium'\n",
      " b'Medium' b'Small' b'Medium' b'Medium' b'Medium' b'Medium' b'Medium'\n",
      " b'Medium' b'Medium' b'Small' b'Medium' b'Medium' b'Medium' b'Medium'\n",
      " b'Medium' b'Small' b'Small' b'Medium' b'Small' b'Medium' b'Medium'\n",
      " b'Medium' b'Small' b'Medium' b'Medium']\n",
      "FurLength           : [b'Short' b'Medium' b'Short' b'Long' b'Short' b'Short' b'Short' b'Short'\n",
      " b'Short' b'Medium' b'Short' b'Medium' b'Medium' b'Short' b'Short'\n",
      " b'Medium' b'Medium' b'Short' b'Short' b'Short' b'Short' b'Medium'\n",
      " b'Medium' b'Short' b'Medium' b'Short' b'Short' b'Medium' b'Medium'\n",
      " b'Medium' b'Medium' b'Medium']\n",
      "Vaccinated          : [b'Yes' b'Yes' b'Yes' b'Yes' b'Yes' b'No' b'Yes' b'No' b'No' b'Not Sure'\n",
      " b'No' b'Yes' b'No' b'No' b'No' b'No' b'No' b'No' b'No' b'Yes' b'No'\n",
      " b'Yes' b'Not Sure' b'Yes' b'No' b'Not Sure' b'No' b'Yes' b'No' b'No'\n",
      " b'Yes' b'No']\n",
      "Sterilized          : [b'Yes' b'Yes' b'No' b'Yes' b'No' b'No' b'Yes' b'No' b'Not Sure'\n",
      " b'Not Sure' b'No' b'No' b'No' b'No' b'No' b'No' b'No' b'No' b'No' b'Yes'\n",
      " b'No' b'No' b'Yes' b'No' b'No' b'Not Sure' b'No' b'No' b'No' b'Yes'\n",
      " b'Yes' b'No']\n",
      "Health              : [b'Healthy' b'Healthy' b'Healthy' b'Healthy' b'Healthy' b'Healthy'\n",
      " b'Healthy' b'Healthy' b'Healthy' b'Minor Injury' b'Healthy' b'Healthy'\n",
      " b'Healthy' b'Healthy' b'Healthy' b'Healthy' b'Minor Injury' b'Healthy'\n",
      " b'Healthy' b'Healthy' b'Healthy' b'Healthy' b'Healthy' b'Healthy'\n",
      " b'Healthy' b'Healthy' b'Healthy' b'Healthy' b'Healthy' b'Healthy'\n",
      " b'Healthy' b'Healthy']\n",
      "Fee                 : [  0   0   0 500   0   0 100   0   0  20 100   0   0   0   0   0   0  30\n",
      "   0   0  50   0 250   0   0   0   0   0   0   0   0   0]\n",
      "PhotoAmt            : [10  2  5  1  2  4  1 10  1  2  1  2  3  1  4  4  7  1  8  4  4  0  3  4\n",
      "  5  1  2  1  1  8 11  5]\n",
      "label               : [1 1 1 1 1 0 0 1 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "# extract one batch to play around\n",
    "batch, label = iter(train_ds).next()\n",
    "\n",
    "# Utility to visualize the dataset structure\n",
    "for key, value in batch.items():\n",
    "    print(f\"{key:20s}: {value}\")\n",
    "print(f\"{'label':20s}: {label}\")\n",
    "\n",
    "# utility to inspect the dataset composition\n",
    "def demo(feature_column):\n",
    "  feature_layer = layers.DenseFeatures(feature_column)\n",
    "  print(feature_layer(batch).numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build feature columns\n",
    "Okay, now I can start to play around by building the feature column. This means that I will combine diffeten features together. First, let's create the groups of basic features that I want to include."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# purely numeric features\n",
    "numeric_features = ['PhotoAmt', \n",
    "                    'Fee']\n",
    "\n",
    "# bucketized features, with buckets to use in a feature:bucket dictionary form\n",
    "bucketized_features = {'Age': [1, 2, 3, 4, 5]}\n",
    "\n",
    "# indicator features\n",
    "indicator_features = ['Type', \n",
    "                      'Color1', \n",
    "                      'Color2', \n",
    "                      'Gender', \n",
    "                      'MaturitySize',\n",
    "                      'FurLength', \n",
    "                      'Vaccinated', \n",
    "                      'Sterilized', \n",
    "                      'Health']\n",
    "\n",
    "# embedded features\n",
    "embedded_features = ['Breed1']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's define the feature columns. Note that you can apply the demo utility on each new_feature separately, or on the overall feature_columns array as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inspect everything\n",
      "WARNING:tensorflow:From /Users/dabol99/opt/anaconda3/envs/py_37/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4276: IndicatorColumn._variable_shape (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "WARNING:tensorflow:From /Users/dabol99/opt/anaconda3/envs/py_37/lib/python3.7/site-packages/tensorflow_core/python/feature_column/feature_column_v2.py:4331: VocabularyListCategoricalColumn._num_buckets (from tensorflow.python.feature_column.feature_column_v2) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "The old _FeatureColumn APIs are being deprecated. Please use the new FeatureColumn APIs instead.\n",
      "[[0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 1. 0. ... 0. 0. 1.]\n",
      " ...\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# function to build the feature columns. The original pandas dataframe is referenced as global variable\n",
    "def build_feature_columns():\n",
    "    feature_columns = []\n",
    "\n",
    "    # add numeric features\n",
    "    for feature_name in numeric_features:\n",
    "        new_feature = feature_column.numeric_column(feature_name)\n",
    "        feature_columns.append(new_feature)\n",
    "\n",
    "    # add bucketized features from numeric    \n",
    "    for feature_name in bucketized_features:\n",
    "        new_feature = feature_column.bucketized_column(feature_column.numeric_column(feature_name),\n",
    "                                                       bucketized_features[feature_name])\n",
    "        feature_columns.append(new_feature)\n",
    "\n",
    "    # add indicator feature\n",
    "    for feature_name in indicator_features:\n",
    "        new_feature_as_categorical = feature_column.categorical_column_with_vocabulary_list(feature_name, dataframe[feature_name].unique())\n",
    "        new_feature_as_indicator   = feature_column.indicator_column(new_feature_as_categorical) \n",
    "        feature_columns.append(new_feature_as_indicator)\n",
    "\n",
    "    # add embedded features\n",
    "    for feature_name in embedded_features:\n",
    "        naive_embedding_size = int(numpy.round(len(dataframe[feature_name].unique())**(0.25)))\n",
    "        new_feature_as_categorical = feature_column.categorical_column_with_vocabulary_list(feature_name, dataframe[feature_name].unique())\n",
    "        new_feature_as_embedding   = feature_column.embedding_column(new_feature_as_categorical, naive_embedding_size)\n",
    "        feature_columns.append(new_feature_as_embedding)\n",
    "\n",
    "    return feature_columns\n",
    "\n",
    "    \n",
    "print('inspect everything')\n",
    "demo(build_feature_columns())\n",
    "# Warnings comes out because conda on macOS can have only tensorflow 2.0.0 an not 2.3.14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural model\n",
    "Now that feature columns are ready, I can now finally train a deep neural network model, albeit a very simple one. Let's start by writing down a funcitons that initialized a model object that I can feed to the hyperparameter tuner. Note that in this function the hyperparameters object does not have specific details on the hyperparameter space. Those are defined within the function initialize_model itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_model(hyperparameters):        \n",
    "    # specify hyperparameter ranges hyperparameter_object \n",
    "    node_units  = hyperparameters.Int('units', min_value = 10, max_value = 50, step = 10)\n",
    "    dropout_val = hyperparameters.Float('dropout', min_value = 0.05, max_value = 0.25, step = 0.05)\n",
    "    optimizer   = hyperparameters.Choice('optimizer', ['adam', 'ftrl'])\n",
    "        \n",
    "    # build input layer from feature columns\n",
    "    input_layer = tensorflow.keras.layers.DenseFeatures(build_feature_columns())    \n",
    "        \n",
    "    # build sequential model\n",
    "    model = tensorflow.keras.Sequential([\n",
    "      input_layer,\n",
    "      layers.Dense(units = node_units, activation='relu'),\n",
    "      layers.Dropout(rate = dropout_val),\n",
    "      layers.Dense(units = node_units, activation='relu'),\n",
    "      layers.Dropout(rate = dropout_val),\n",
    "      # sigmoid layer to perfomr classification tasks\n",
    "      layers.Dense(1,  activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # compile model\n",
    "    model.compile(optimizer = optimizer,\n",
    "                  loss = tensorflow.keras.losses.BinaryCrossentropy(from_logits = True),\n",
    "                  metrics = ['accuracy',\n",
    "                            tensorflow.keras.metrics.Precision(name='precision'),\n",
    "                            tensorflow.keras.metrics.Recall(name='recall')])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now use this initializer function to set up an hyperparameter tuner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project logs/hyperparameter_tuning/oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from logs/hyperparameter_tuning/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tuner = kerastuner.Hyperband(initialize_model,\n",
    "                             objective = 'val_accuracy', \n",
    "                             max_epochs = 10,\n",
    "                             factor = 3,\n",
    "                             directory = 'logs',\n",
    "                             project_name = 'hyperparameter_tuning')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now run the hyperparameter tuner to find the best hyperparameter configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "tuner.search(train_ds, epochs = 20, validation_data = val_ds, verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the log from the tuner, we can now find the best parameter and train the corresponding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best model:\n",
      "nodes:     30\n",
      "dropout:   0.15000000000000002\n",
      "optimizer: adam\n"
     ]
    }
   ],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "print('best model:')\n",
    "print(f\"\"\"nodes:     {best_hps.get('units')}\"\"\")\n",
    "print(f\"\"\"dropout:   {best_hps.get('dropout')}\"\"\")\n",
    "print(f\"\"\"optimizer: {best_hps.get('optimizer')}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can now train the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7ff26ccbcd50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model with the optimal hyperparameters and train it on the data\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "model.fit(train_ds, epochs = 10, validation_data = val_ds, verbose = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the trained model, we can estimate now the performances of the model. I know that I could play with other hyperparameters in this dataset, such as the number of layers to implement, or the size of the embedding for breeds, or joint variables. Such complex optimization is whereas the model evaluation in the test set is outside the scope of this example, so I will move toward validation on the test set instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58/58 [==============================] - ETA: 0s - loss: 0.6952 - accuracy: 0.6250 - precision: 0.6154 - recall: 0.888 - ETA: 0s - loss: 0.5611 - accuracy: 0.7301 - precision: 0.7744 - recall: 0.891 - ETA: 0s - loss: 0.5630 - accuracy: 0.7361 - precision: 0.7766 - recall: 0.892 - ETA: 0s - loss: 0.5529 - accuracy: 0.7380 - precision: 0.7878 - recall: 0.882 - ETA: 0s - loss: 0.5554 - accuracy: 0.7330 - precision: 0.7855 - recall: 0.877 - ETA: 0s - loss: 0.5518 - accuracy: 0.7386 - precision: 0.7900 - recall: 0.881 - ETA: 0s - loss: 0.5468 - accuracy: 0.7476 - precision: 0.7975 - recall: 0.884 - 0s 6ms/step - loss: 0.5487 - accuracy: 0.7443 - precision: 0.7950 - recall: 0.8812\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.5544 - accuracy: 0.6875 - precision: 0.8182 - recall: 0.750 - ETA: 0s - loss: 0.5494 - accuracy: 0.7437 - precision: 0.8000 - recall: 0.868 - ETA: 0s - loss: 0.5460 - accuracy: 0.7615 - precision: 0.8045 - recall: 0.889 - ETA: 0s - loss: 0.5528 - accuracy: 0.7533 - precision: 0.7956 - recall: 0.887 - ETA: 0s - loss: 0.5472 - accuracy: 0.7576 - precision: 0.8002 - recall: 0.890 - ETA: 0s - loss: 0.5544 - accuracy: 0.7493 - precision: 0.7902 - recall: 0.890 - ETA: 0s - loss: 0.5563 - accuracy: 0.7398 - precision: 0.7860 - recall: 0.880 - ETA: 0s - loss: 0.5605 - accuracy: 0.7351 - precision: 0.7816 - recall: 0.878 - 0s 6ms/step - loss: 0.5577 - accuracy: 0.7370 - precision: 0.7824 - recall: 0.8806\n"
     ]
    }
   ],
   "source": [
    "performances_on_validation = model.evaluate(val_ds)\n",
    "performances_on_test = model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performances on validation:\n",
      "Accuracy:  0.744312047958374\n",
      "Precision: 0.7949735522270203\n",
      "Recall:    0.8812316656112671\n",
      "\n",
      "Performances on test:\n",
      "Accuracy:  0.7370017170906067\n",
      "Precision: 0.7824000120162964\n",
      "Recall:    0.8805522322654724\n"
     ]
    }
   ],
   "source": [
    "print(f\"\"\"Performances on validation:\n",
    "Accuracy:  {performances_on_validation[1]}\n",
    "Precision: {performances_on_validation[2]}\n",
    "Recall:    {performances_on_validation[3]}\n",
    "\n",
    "Performances on test:\n",
    "Accuracy:  {performances_on_test[1]}\n",
    "Precision: {performances_on_test[2]}\n",
    "Recall:    {performances_on_test[3]}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performances on the validation and the test set seems in agreement, so the predictive model seems to generalize quite well. This means that I can deploy it. This requires first to save the model for production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/dabol99/opt/anaconda3/envs/py_37/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: /Users/dabol99/Documents/DS projects/Animal_adoptions/animal_adoption_model/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(os.path.join(os.getcwd(), 'animal_adoption_model'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which I will be able to deploy on GCP the day I want to pay for their services. Yay!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
